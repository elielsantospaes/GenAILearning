{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3650b81-f2eb-40e0-8f55-42c21e92aafd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08a0fc5e-8078-4703-959a-f872cdd63a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction to Embeddings for RAG Chatbots\n",
    "\n",
    "#### ğŸ”¹ What Are Embeddings?\n",
    "\n",
    "In the context of AI and RAG (Retrieval-Augmented Generation), **embeddings** are a way to represent text (or any data) as **vectors of real numbers**.\n",
    "\n",
    "The goal is to **capture the meaning of text** in such a way that:\n",
    "- Similar meanings are represented by **vectors that are close** together.\n",
    "- Dissimilar meanings are **far apart**.\n",
    "\n",
    "This is essential in RAG pipelines: you embed documents and queries into the same vector space, then retrieve the most relevant docs by comparing vectors (e.g., via cosine similarity).\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ”¹ How Do Embedding Techniques Work?\n",
    "\n",
    "Letâ€™s take a sentence:  \n",
    "> â€œThe cat sat on the mat.â€\n",
    "\n",
    "An embedding model (like OpenAI's `text-embedding-3-small`, or `sentence-transformers`, or `BERT`) will:\n",
    "1. **Tokenize** the input (e.g., into words or subwords).\n",
    "2. **Process the tokens** through a neural network (often a transformer).\n",
    "3. Output a **vector of floats**, such as:\n",
    "\n",
    "\n",
    "Each number in the vector doesn't have an interpretable meaning on its own. Instead:\n",
    "- The entire vector captures **abstract linguistic and semantic features**.\n",
    "- For example, some dimensions might implicitly capture notions like:\n",
    "  - Formality  \n",
    "  - Topic domain  \n",
    "  - Sentiment  \n",
    "  - Subject-object relationships  \n",
    "  - Temporal reference\n",
    "\n",
    "These are **not explicitly labeled** â€” they are learned during training on massive datasets.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ”¹ Why Is the Dimension Important?\n",
    "\n",
    "The **dimension** of an embedding (e.g., 384, 768, 1024, etc.) refers to the **length of the vector**.\n",
    "\n",
    "- Higher dimensions can capture **more nuance** and **fine-grained meaning**.\n",
    "- But they come at the cost of:\n",
    "  - **Higher computational resources**\n",
    "  - **More memory** for storing the embeddings\n",
    "  - Potential **overfitting or redundancy** if the task doesnâ€™t need that much expressiveness\n",
    "\n",
    "#### ğŸ§  Analogy:\n",
    "Think of embedding vectors like coordinates in a city:\n",
    "- Each number is like a street number or GPS coordinate.\n",
    "- The more dimensions, the more **precise** your location in â€œsemantic spaceâ€.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ”¹ What Does Each Number in the Vector Mean?\n",
    "\n",
    "Hereâ€™s the honest truth:  \n",
    "**You canâ€™t interpret each number directly.**  \n",
    "They're **latent features**, discovered by the neural network to best represent meaning.\n",
    "\n",
    "> For instance, `0.132` in dimension 42 might reflect â€œslightly positive sentimentâ€ or â€œpresence of animal conceptsâ€ â€” but we donâ€™t know for sure.\n",
    "\n",
    "Researchers use **vector algebra** to explore what embeddings â€œmeanâ€:\n",
    "- Example:  \n",
    "  `embedding(\"king\") - embedding(\"man\") + embedding(\"woman\") â‰ˆ embedding(\"queen\")`\n",
    "\n",
    "This shows that **relationships** are preserved in vector space. These embedding algebra expressions are not just fun â€” theyâ€™re powerful tools for understanding how meaning is encoded in vector space. You can definitely use similar examples to check whether an embedding model is working well for your domain (especially before deploying in a RAG system).\n",
    "\n",
    "#### How These Examples Work ğŸ”\n",
    "Each example takes advantage of the fact that semantic relationships are preserved in the vector space.\n",
    "\n",
    "The general idea:\n",
    "\n",
    "embedding(A) - embedding(B) + embedding(C) â‰ˆ embedding(D)\n",
    "Where:\n",
    "\n",
    "A and B are related in a certain way.\n",
    "\n",
    "C and D are expected to have the same kind of relationship.\n",
    "\n",
    "##### 10 Semantic Analogy Examples to Try with Embeddings\n",
    "**ğŸ° Gender Analogies**   \n",
    "embedding(\"king\") - embedding(\"man\") + embedding(\"woman\") â‰ˆ embedding(\"queen\")\n",
    "\n",
    "embedding(\"prince\") - embedding(\"man\") + embedding(\"woman\") â‰ˆ embedding(\"princess\")\n",
    "\n",
    "embedding(\"actor\") - embedding(\"man\") + embedding(\"woman\") â‰ˆ embedding(\"actress\")\n",
    "\n",
    "**ğŸŒ Countryâ€“Capital Analogies**    \n",
    "embedding(\"France\") - embedding(\"Paris\") + embedding(\"Rome\") â‰ˆ embedding(\"Italy\")\n",
    "(Or inverse: France is to Paris as Italy is to Rome)\n",
    "\n",
    "embedding(\"Japan\") - embedding(\"Tokyo\") + embedding(\"London\") â‰ˆ embedding(\"UK\")\n",
    "**\n",
    "ğŸ¢ Companyâ€“Product Analogies**    \n",
    "embedding(\"Apple\") - embedding(\"iPhone\") + embedding(\"Galaxy\") â‰ˆ embedding(\"Samsung\")\n",
    "\n",
    "embedding(\"Microsoft\") - embedding(\"Windows\") + embedding(\"macOS\") â‰ˆ embedding(\"Apple\")\n",
    "\n",
    "**ğŸ§­ Singularâ€“Plural Analogies**    \n",
    "embedding(\"cat\") - embedding(\"cats\") + embedding(\"dogs\") â‰ˆ embedding(\"dog\")\n",
    "\n",
    "embedding(\"child\") - embedding(\"children\") + embedding(\"adults\") â‰ˆ embedding(\"adult\")\n",
    "\n",
    "**ğŸ“ Degree of Comparison**    \n",
    "embedding(\"fast\") - embedding(\"faster\") + embedding(\"stronger\") â‰ˆ embedding(\"strong\")\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "\n",
    "| Concept           | Description                                                 |\n",
    "|------------------|-------------------------------------------------------------|\n",
    "| **Embedding**     | A vector that represents meaning of a word, sentence, or doc |\n",
    "| **Dimension**     | Length of vector (e.g., 384, 768), determines expressiveness |\n",
    "| **Each Number**   | Latent, abstract features learned by the model              |\n",
    "| **Use in RAG**    | Match query/document embeddings via similarity to retrieve relevant info |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e7378a8-664f-4ebb-9980-5d1b936a2abc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "embedding_studies",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
